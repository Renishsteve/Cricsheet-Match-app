ğŸ“„ README:

ğŸ Cricsheet Match Data Analysis Project

This project is an end-to-end cricket analytics pipeline that downloads real match data from Cricsheet, processes it, stores it in a database, runs analytical SQL queries, and generates visualizations â€” all in one click.


----------

ğŸ“Œ Prjct Summary:

1. Download JSON match files for multiple formats (Test, ODI, T20, IPL) using Selenium.


2. Transform raw JSON into structured CSV and Parquet files.


3. Load the processed data into a SQLite database.


4. Analyze the data by running 20 predefined SQL queries.


5. Visualize the results with automatic EDA charts.




---

ğŸ“‚ Project Structure

Cricsheet-match-app/
â”‚
â”œâ”€â”€ scrapper.py               # Downloads JSON match data into data/raw
â”œâ”€â”€ transform_json.py         # Converts JSON to CSV/Parquet
â”œâ”€â”€ load_to_db.py              # Loads CSV data into SQLite DB
â”œâ”€â”€ run_sql_analyses.py        # Runs queries.sql and saves results to sql_results.json
â”œâ”€â”€ eda.py                     # Creates charts from sql_results.json
â”œâ”€â”€ run_all.py                 # Master script to run everything in sequence
â”œâ”€â”€ queries.sql                # List of 20 SQL queries for analysis
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Downloaded JSON files
â”‚   â””â”€â”€ processed/             # Flattened CSV and Parquet files
â”‚
â”œâ”€â”€ cricket.db                 # SQLite database (created after loading)
â”œâ”€â”€ sql_results.json           # Results of SQL queries
â”œâ”€â”€ figures/                   # Generated charts
â”‚
â””â”€â”€ README.md                  # This file


---

âš™ï¸ Requirements



pip install pandas matplotlib seaborn sqlalchemy selenium


---

ğŸš€ Run Procedure:



The easiest way to run everything is with:

python run_all.py

This will do as below:-

1. Step 0: Scrape latest match data (scrapper.py)


2. Step 1: Transform JSON â†’ CSV/Parquet (transform_json.py)


3. Step 2: Load CSV into SQLite database (load_to_db.py)


4. Step 3: Run all SQL queries (run_sql_analyses.py)


5. Step 4: Wait for sql_results.json to be ready


6. Step 5: Generate visualizations (eda.py)




---

Run Steps Manually

If you want to run each step individually:

# Step 0: Scrape data
python scrapper.py

# Step 1: Transform JSONs
python transform_json.py --input-dir data/raw --output data/processed/flattened.parquet --also-csv

# Step 2: Load into DB
python load_to_db.py --csv-prefix data/processed/flattened --db sqlite:///cricket.db

# Step 3: Run SQL analyses
python run_sql_analyses.py --db sqlite:///cricket.db --sql-file queries.sql --out sql_results.json

# Step 4: Generate EDA plots
python eda.py --sql-results-file sql_results.json --out-dir figures


---

ğŸ“Š Output

sql_results.json â€” Stores the output of all SQL queries in JSON format.

figures/ â€” Contains .png charts for each query.

cricket.db â€” SQLite database with cleaned cricket match data.